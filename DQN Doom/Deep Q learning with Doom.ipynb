{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q learning with Doom üïπÔ∏è\n",
    "In this notebook we'll implement an agent <b>that plays Doom by using a Deep Q learning architecture.</b> <br>\n",
    "Our agent playing Doom:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/doom_reduced.gif\" style=\"max-width: 600px;\" alt=\"Deep Q learning with Doom\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is part of the Free Deep Reinforcement Course üìù\n",
    "<img src=\"https://simoninithomas.github.io/Deep_reinforcement_learning_Course/assets/img/preview.jpg\" alt=\"Deep Reinforcement Course\" style=\"width: 500px;\"/>\n",
    "\n",
    "<p> Deep Reinforcement Learning Course is a free series of blog posts about Deep Reinforcement Learning, where we'll learn the main algorithms, <b>and how to implement them in Tensorflow.</b></p>\n",
    "\n",
    "<p>The goal of these articles is to <b>explain step by step from the big picture</b> and the mathematical details behind it, to the implementation with Tensorflow </p>\n",
    "\n",
    "\n",
    "<a href=\"https://simoninithomas.github.io/Deep_reinforcement_learning_Course/\">Syllabus</a><br>\n",
    "<a href=\"https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419\">Part 0: Introduction to Reinforcement Learning </a><br>\n",
    "<a href=\"\"> Part 1: Q-learning with FrozenLake</a><br>\n",
    "<a href=\"\"> Part 2: Deep Q-learning with Doom</a><br>\n",
    "<a href=\"\"> Part 3: Policy Gradients with Doom </a><br>\n",
    "\n",
    "## Checklist üìù\n",
    "- To launch tensorboard : `tensorboard --logdir=/tensorboard/dqn/1`\n",
    "- ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è You need to download vizdoom and place the folder in the repos.\n",
    "- If don't want to train, you must change **training to False** (in hyperparameters step). \n",
    "\n",
    "\n",
    "## Any questions üë®‚Äçüíª\n",
    "<p> If you have any questions, feel free to ask me: </p>\n",
    "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
    "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
    "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
    "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
    "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
    "    \n",
    "\n",
    "## How to help  üôå\n",
    "3 ways:\n",
    "- **Clap our articles a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared\n",
    "- **Share and speak about our articles**: By sharing our articles you help us to spread the word.\n",
    "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
    "<br>\n",
    "\n",
    "## Important note ü§î\n",
    "Some problems with jupyter notebook and GPU service forced me to run this notebook on my computer, **you can too but it's easier with GPUs**. \n",
    "\n",
    "<b> You can run it on your computer but it's better to run it on GPU based services </b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
    "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
    "<br>\n",
    "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
    "\n",
    "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "- Now that we imported the libraries/dependencies, we will create our environment.\n",
    "- Doom environment takes:\n",
    "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
    "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
    "- Note: We have 3 possible actions `[[0,0,1], [1,0,0], [0,1,0]]` so we don't need to do one hot encoding (thanks to < a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out. \n",
    "\n",
    "### Our environment\n",
    "<img src=\"assets/doom.png\" style=\"max-width:500px;\" alt=\"Doom\"/>\n",
    "                                    \n",
    "- A monster is spawned **randomly somewhere along the opposite wall**. \n",
    "- Player can only go **left/right and shoot**. \n",
    "- 1 hit is enough **to kill the monster**. \n",
    "- Episode finishes when **monster is killed or on timeout (300)**.\n",
    "<br><br>\n",
    "REWARDS:\n",
    "\n",
    "- +101 for killing the monster \n",
    "- -5 for missing \n",
    "- Episode ends after killing the monster or on timeout.\n",
    "- living reward = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we create our environment\n",
    "\"\"\"\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case basic scenario)\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # Here our possible actions\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions\n",
    "       \n",
    "\"\"\"\n",
    "Here we performing random action to test the environment\n",
    "\"\"\"\n",
    "def test_environment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.init()\n",
    "    shoot = [0, 0, 1]\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    actions = [shoot, left, right]\n",
    "\n",
    "    episodes = 10\n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(actions)\n",
    "            print(action)\n",
    "            reward = game.make_action(action)\n",
    "            print (\"\\treward:\", reward)\n",
    "            time.sleep(0.02)\n",
    "        print (\"Result:\", game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set up our hyperparameters ‚öóÔ∏è\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
    "\n",
    "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
    "- Then, you'll add the training hyperparameters when you implement the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [84,84,4]      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size()              # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.0002      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 5000        # Total episodes for training\n",
    "max_steps = 100              # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.99\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size\n",
    "memory_size = 50000\n",
    "\n",
    "### PREPROCESSING HYPERPARAMETERS\n",
    "stack_size = 4\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Define the preprocessing functions ‚öôÔ∏è\n",
    "### preprocess_frame\n",
    "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
    "<br><br>\n",
    "Our steps:\n",
    "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
    "- Crop the screen (in our case we remove the roof because it contains no information)\n",
    "- We normalize pixel values\n",
    "- Finally we resize the preprocessed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess_frame:\n",
    "    Take a frame.\n",
    "    Resize it.\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "        \n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "    Normalize it.\n",
    "    \n",
    "    return preprocessed_frame\n",
    "    \n",
    "    \"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Greyscale frame already done in our vizdoom config\n",
    "    # x = np.mean(frame,-1)\n",
    "    \n",
    "    # Crop the screen (remove the roof because it contains no information)\n",
    "    cropped_frame = frame[30:-10,30:-30]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84,84])\n",
    "    \n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack_frames\n",
    "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
    "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
    "- First we preprocess frame\n",
    "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
    "- Finally we **build the stacked state**\n",
    "\n",
    "This is how work stack:\n",
    "- For the first frame, we feed the other 3 with blank frames\n",
    "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
    "- And so on\n",
    "<img src=\"assets\\stack.png\" alt=\"stack\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "        \n",
    "    # Append frame to deque, automatically removes the oldest frame\n",
    "    stacked_frames.append(frame)\n",
    "       \n",
    "    # Build the stacked state (first dimension specifies different frames)\n",
    "    stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    \n",
    "    return stacked_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Deep Q-learning Neural Network model üß†\n",
    "<img src=\"assets/model.png\" alt=\"Model\" />\n",
    "This is our Deep Q-learning model:\n",
    "- We take a stack of 4 frames as input\n",
    "- It passes through 3 convnets\n",
    "- Then it is flatened\n",
    "- Finally it passes through 2 FC layers\n",
    "- It outputs a Q value for each actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "            # [None, 84, 84, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name=\"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\"\n",
    "            First convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            # Input is 84x84x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm1')\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "            ## --> [20, 20, 32]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Second convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "        \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm2')\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "            ## --> [9, 9, 64]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Third convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "        \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm3')\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "            ## --> [3, 3, 128]\n",
    "            \n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            ## --> [1152]\n",
    "            \n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"fc1\")\n",
    "            \n",
    "            \n",
    "            self.output = tf.layers.dense(inputs = self.fc, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units = 3, \n",
    "                                        activation=None)\n",
    "\n",
    "  \n",
    "            # Q is our predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            \n",
    "            # The loss is the difference between our predicted Q_values and the Q_target\n",
    "            # Sum(Qtarget - Q)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Experience Replay üîÅ\n",
    "Now that we create our Neural Network, **we need to implement the Experience Replay method.** <br><br>\n",
    "Here we'll create the Memory object that creates a deque.A deque (double ended queue) is a data type that **removes the oldest element each time that you add a new element.**\n",
    "\n",
    "This part was taken from Udacity : <a href=\"https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb\" Cartpole DQN</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll **deal with the empty memory problem**: we pre-populate our memory by taking random actions and storing the experience (state, action, reward, new_state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\Anaconda3\\envs\\gameplai\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "# Instantiate memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state = stack_frames(stacked_frames, state)\n",
    "    \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    \n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state = stack_frames(stacked_frames, next_state)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/dqn/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train our Agent üèÉ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Our algorithm:\n",
    "<br>\n",
    "* Initialize the weights\n",
    "* Init the environment\n",
    "* Initialize the decay rate (that will use to reduce epsilon) \n",
    "<br><br>\n",
    "* **For** episode to max_episode **do** \n",
    "    * Make new episode\n",
    "    * Set step to 0\n",
    "    * Observe the first state $s_0$\n",
    "    <br><br>\n",
    "    * **While** step < max_steps **do**:\n",
    "        * Increase decay_rate\n",
    "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
    "        * Set $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma \\max_{a'}{Q(s', a')}$\n",
    "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "    * **endfor**\n",
    "    <br><br>\n",
    "* **endfor**\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\Anaconda3\\envs\\gameplai\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 95.0 Training loss: 159.8951 Explore P: 0.9994\n",
      "Model Saved\n",
      "Episode: 1 Total reward: 91.0 Training loss: 314.2269 Explore P: 0.9984\n",
      "Episode: 2 Total reward: 25.0 Training loss: 249.3162 Explore P: 0.9924\n",
      "Episode: 3 Total reward: 95.0 Training loss: 91.9304 Explore P: 0.9918\n",
      "Episode: 5 Total reward: 93.0 Training loss: 20.8968 Explore P: 0.9813\n",
      "Model Saved\n",
      "Episode: 6 Total reward: 94.0 Training loss: 30.8907 Explore P: 0.9806\n",
      "Episode: 7 Total reward: 95.0 Training loss: 241.0443 Explore P: 0.9800\n",
      "Episode: 10 Total reward: 95.0 Training loss: 33.5293 Explore P: 0.9602\n",
      "Model Saved\n",
      "Episode: 13 Total reward: 95.0 Training loss: 9.7165 Explore P: 0.9409\n",
      "Episode: 15 Total reward: 95.0 Training loss: 12.4297 Explore P: 0.9310\n",
      "Model Saved\n",
      "Episode: 16 Total reward: 94.0 Training loss: 11.5098 Explore P: 0.9304\n",
      "Episode: 18 Total reward: 15.0 Training loss: 37.0864 Explore P: 0.9148\n",
      "Episode: 20 Total reward: 69.0 Training loss: 16.4923 Explore P: 0.9034\n",
      "Model Saved\n",
      "Episode: 21 Total reward: 90.0 Training loss: 8.9101 Explore P: 0.9024\n",
      "Episode: 23 Total reward: 95.0 Training loss: 25.2000 Explore P: 0.8930\n",
      "Episode: 25 Total reward: 76.0 Training loss: 15.5866 Explore P: 0.8824\n",
      "Model Saved\n",
      "Episode: 26 Total reward: 15.0 Training loss: 38.3781 Explore P: 0.8763\n",
      "Episode: 27 Total reward: 95.0 Training loss: 26.9352 Explore P: 0.8758\n",
      "Episode: 28 Total reward: 94.0 Training loss: 9.0251 Explore P: 0.8752\n",
      "Episode: 29 Total reward: 95.0 Training loss: 18.6325 Explore P: 0.8746\n",
      "Model Saved\n",
      "Episode: 31 Total reward: 95.0 Training loss: 12.1557 Explore P: 0.8655\n",
      "Episode: 35 Total reward: 90.0 Training loss: 14.3587 Explore P: 0.8393\n",
      "Model Saved\n",
      "Episode: 36 Total reward: 95.0 Training loss: 10.5791 Explore P: 0.8388\n",
      "Episode: 39 Total reward: 93.0 Training loss: 18.6541 Explore P: 0.8218\n",
      "Model Saved\n",
      "Episode: 42 Total reward: 91.0 Training loss: 26.6867 Explore P: 0.8049\n",
      "Model Saved\n",
      "Episode: 47 Total reward: 93.0 Training loss: 9.8341 Explore P: 0.7731\n",
      "Episode: 48 Total reward: 18.0 Training loss: 8.1705 Explore P: 0.7679\n",
      "Episode: 49 Total reward: 93.0 Training loss: 8.0922 Explore P: 0.7673\n",
      "Model Saved\n",
      "Episode: 51 Total reward: 95.0 Training loss: 6.4622 Explore P: 0.7593\n",
      "Episode: 52 Total reward: 92.0 Training loss: 8.7915 Explore P: 0.7587\n",
      "Episode: 53 Total reward: 95.0 Training loss: 14.3846 Explore P: 0.7582\n",
      "Episode: 54 Total reward: 94.0 Training loss: 9.6200 Explore P: 0.7577\n",
      "Episode: 55 Total reward: 93.0 Training loss: 11.9846 Explore P: 0.7571\n",
      "Model Saved\n",
      "Episode: 57 Total reward: 93.0 Training loss: 11.7084 Explore P: 0.7491\n",
      "Episode: 58 Total reward: 56.0 Training loss: 6.2880 Explore P: 0.7461\n",
      "Episode: 59 Total reward: 82.0 Training loss: 14.7713 Explore P: 0.7447\n",
      "Episode: 60 Total reward: 34.0 Training loss: 6.8712 Explore P: 0.7406\n",
      "Model Saved\n",
      "Episode: 61 Total reward: 95.0 Training loss: 14.4346 Explore P: 0.7401\n",
      "Episode: 64 Total reward: 11.0 Training loss: 11.7134 Explore P: 0.7203\n",
      "Model Saved\n",
      "Episode: 66 Total reward: 95.0 Training loss: 13.1062 Explore P: 0.7128\n",
      "Episode: 67 Total reward: 94.0 Training loss: 10.0990 Explore P: 0.7123\n",
      "Episode: 69 Total reward: 92.0 Training loss: 11.9453 Explore P: 0.7047\n",
      "Episode: 70 Total reward: 72.0 Training loss: 53.4186 Explore P: 0.7031\n",
      "Model Saved\n",
      "Episode: 72 Total reward: 95.0 Training loss: 4.5559 Explore P: 0.6957\n",
      "Episode: 73 Total reward: 19.0 Training loss: 7.8003 Explore P: 0.6912\n",
      "Episode: 74 Total reward: 22.0 Training loss: 14.2777 Explore P: 0.6868\n",
      "Episode: 75 Total reward: 90.0 Training loss: 8.8209 Explore P: 0.6861\n",
      "Model Saved\n",
      "Episode: 76 Total reward: 87.0 Training loss: 11.2239 Explore P: 0.6851\n",
      "Episode: 77 Total reward: 91.0 Training loss: 10.9673 Explore P: 0.6845\n",
      "Episode: 79 Total reward: 95.0 Training loss: 11.8627 Explore P: 0.6773\n",
      "Model Saved\n",
      "Episode: 81 Total reward: 95.0 Training loss: 10.8394 Explore P: 0.6703\n",
      "Episode: 82 Total reward: 95.0 Training loss: 14.4153 Explore P: 0.6699\n",
      "Episode: 83 Total reward: 93.0 Training loss: 10.0134 Explore P: 0.6694\n",
      "Episode: 85 Total reward: 92.0 Training loss: 22.4809 Explore P: 0.6622\n",
      "Model Saved\n",
      "Episode: 88 Total reward: 5.0 Training loss: 12.2209 Explore P: 0.6445\n",
      "Episode: 89 Total reward: 95.0 Training loss: 9.2033 Explore P: 0.6441\n",
      "Model Saved\n",
      "Episode: 92 Total reward: 93.0 Training loss: 22.3943 Explore P: 0.6310\n",
      "Episode: 93 Total reward: 8.0 Training loss: 6.7298 Explore P: 0.6262\n",
      "Episode: 94 Total reward: 8.0 Training loss: 4.8743 Explore P: 0.6217\n",
      "Model Saved\n",
      "Episode: 96 Total reward: 86.0 Training loss: 12.7095 Explore P: 0.6147\n",
      "Episode: 97 Total reward: 91.0 Training loss: 5.9015 Explore P: 0.6141\n",
      "Episode: 98 Total reward: 64.0 Training loss: 17.4857 Explore P: 0.6122\n",
      "Episode: 100 Total reward: 92.0 Training loss: 7.7967 Explore P: 0.6057\n",
      "Model Saved\n",
      "Episode: 101 Total reward: 68.0 Training loss: 7.4096 Explore P: 0.6040\n",
      "Episode: 102 Total reward: 89.0 Training loss: 13.0343 Explore P: 0.6033\n",
      "Episode: 103 Total reward: 61.0 Training loss: 15.6852 Explore P: 0.6012\n",
      "Episode: 104 Total reward: 92.0 Training loss: 10.2076 Explore P: 0.6007\n",
      "Episode: 105 Total reward: 95.0 Training loss: 14.1960 Explore P: 0.6003\n",
      "Model Saved\n",
      "Episode: 107 Total reward: 50.0 Training loss: 10.5710 Explore P: 0.5921\n",
      "Episode: 108 Total reward: 94.0 Training loss: 8.1464 Explore P: 0.5917\n",
      "Episode: 110 Total reward: 95.0 Training loss: 18.1634 Explore P: 0.5855\n",
      "Model Saved\n",
      "Episode: 111 Total reward: 88.0 Training loss: 5.4761 Explore P: 0.5848\n",
      "Episode: 112 Total reward: 92.0 Training loss: 4.9826 Explore P: 0.5843\n",
      "Episode: 113 Total reward: 65.0 Training loss: 8.6424 Explore P: 0.5825\n",
      "Episode: 115 Total reward: 95.0 Training loss: 6.3347 Explore P: 0.5765\n",
      "Model Saved\n",
      "Episode: 116 Total reward: 67.0 Training loss: 9.7962 Explore P: 0.5748\n",
      "Episode: 117 Total reward: 88.0 Training loss: 9.2029 Explore P: 0.5741\n",
      "Episode: 118 Total reward: 93.0 Training loss: 9.6419 Explore P: 0.5736\n",
      "Episode: 119 Total reward: 94.0 Training loss: 8.1946 Explore P: 0.5732\n",
      "Episode: 120 Total reward: 89.0 Training loss: 8.3639 Explore P: 0.5726\n",
      "Model Saved\n",
      "Episode: 121 Total reward: 69.0 Training loss: 12.6917 Explore P: 0.5710\n",
      "Episode: 122 Total reward: 95.0 Training loss: 4.7109 Explore P: 0.5707\n",
      "Episode: 123 Total reward: 15.0 Training loss: 9.8034 Explore P: 0.5667\n",
      "Episode: 125 Total reward: 95.0 Training loss: 11.1381 Explore P: 0.5609\n",
      "Model Saved\n",
      "Episode: 126 Total reward: 87.0 Training loss: 4.4927 Explore P: 0.5601\n",
      "Episode: 127 Total reward: 92.0 Training loss: 16.6166 Explore P: 0.5596\n",
      "Episode: 128 Total reward: 93.0 Training loss: 10.0827 Explore P: 0.5592\n",
      "Episode: 129 Total reward: 90.0 Training loss: 8.9343 Explore P: 0.5586\n",
      "Episode: 130 Total reward: 87.0 Training loss: 11.7824 Explore P: 0.5578\n",
      "Model Saved\n",
      "Episode: 131 Total reward: 90.0 Training loss: 5.3598 Explore P: 0.5572\n",
      "Episode: 132 Total reward: 86.0 Training loss: 7.1476 Explore P: 0.5564\n",
      "Episode: 133 Total reward: 12.0 Training loss: 7.7282 Explore P: 0.5523\n",
      "Episode: 134 Total reward: 94.0 Training loss: 10.5351 Explore P: 0.5520\n",
      "Episode: 135 Total reward: 95.0 Training loss: 8.3407 Explore P: 0.5516\n",
      "Model Saved\n",
      "Episode: 136 Total reward: 90.0 Training loss: 6.7661 Explore P: 0.5510\n",
      "Episode: 137 Total reward: 94.0 Training loss: 8.4989 Explore P: 0.5507\n",
      "Episode: 138 Total reward: 91.0 Training loss: 5.1357 Explore P: 0.5501\n",
      "Episode: 139 Total reward: 47.0 Training loss: 5.5490 Explore P: 0.5478\n",
      "Episode: 140 Total reward: 10.0 Training loss: 17.7785 Explore P: 0.5437\n",
      "Model Saved\n",
      "Episode: 141 Total reward: 94.0 Training loss: 12.0698 Explore P: 0.5433\n",
      "Episode: 142 Total reward: 93.0 Training loss: 4.6766 Explore P: 0.5429\n",
      "Episode: 143 Total reward: 93.0 Training loss: 13.6549 Explore P: 0.5425\n",
      "Episode: 144 Total reward: 89.0 Training loss: 10.9812 Explore P: 0.5418\n",
      "Model Saved\n",
      "Episode: 146 Total reward: 42.0 Training loss: 8.7968 Explore P: 0.5340\n",
      "Episode: 147 Total reward: 93.0 Training loss: 11.5728 Explore P: 0.5335\n",
      "Episode: 149 Total reward: 54.0 Training loss: 6.9453 Explore P: 0.5262\n",
      "Episode: 150 Total reward: 13.0 Training loss: 8.7911 Explore P: 0.5224\n",
      "Model Saved\n",
      "Episode: 151 Total reward: 94.0 Training loss: 10.5494 Explore P: 0.5220\n",
      "Episode: 152 Total reward: 94.0 Training loss: 8.1495 Explore P: 0.5217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 153 Total reward: 95.0 Training loss: 9.9128 Explore P: 0.5214\n",
      "Episode: 154 Total reward: 88.0 Training loss: 5.9731 Explore P: 0.5207\n",
      "Episode: 155 Total reward: 93.0 Training loss: 8.5361 Explore P: 0.5203\n",
      "Model Saved\n",
      "Episode: 156 Total reward: 95.0 Training loss: 9.6579 Explore P: 0.5200\n",
      "Episode: 158 Total reward: 57.0 Training loss: 12.6537 Explore P: 0.5132\n",
      "Episode: 159 Total reward: 88.0 Training loss: 7.6906 Explore P: 0.5126\n",
      "Episode: 160 Total reward: 88.0 Training loss: 11.4646 Explore P: 0.5119\n",
      "Model Saved\n",
      "Episode: 162 Total reward: 93.0 Training loss: 8.9778 Explore P: 0.5065\n",
      "Episode: 163 Total reward: 94.0 Training loss: 13.8677 Explore P: 0.5062\n",
      "Episode: 164 Total reward: 93.0 Training loss: 8.4455 Explore P: 0.5058\n",
      "Episode: 165 Total reward: 93.0 Training loss: 8.1312 Explore P: 0.5054\n",
      "Model Saved\n",
      "Episode: 166 Total reward: 95.0 Training loss: 13.6543 Explore P: 0.5051\n",
      "Episode: 167 Total reward: 94.0 Training loss: 6.8575 Explore P: 0.5047\n",
      "Episode: 168 Total reward: 94.0 Training loss: 29.0103 Explore P: 0.5044\n",
      "Episode: 170 Total reward: 65.0 Training loss: 7.3949 Explore P: 0.4979\n",
      "Model Saved\n",
      "Episode: 171 Total reward: 74.0 Training loss: 6.1278 Explore P: 0.4969\n",
      "Episode: 172 Total reward: 94.0 Training loss: 7.3688 Explore P: 0.4965\n",
      "Episode: 174 Total reward: 95.0 Training loss: 11.5271 Explore P: 0.4914\n",
      "Episode: 175 Total reward: 95.0 Training loss: 7.6468 Explore P: 0.4911\n",
      "Model Saved\n",
      "Episode: 176 Total reward: 93.0 Training loss: 7.9869 Explore P: 0.4907\n",
      "Episode: 177 Total reward: 48.0 Training loss: 13.5864 Explore P: 0.4887\n",
      "Episode: 178 Total reward: 88.0 Training loss: 16.1331 Explore P: 0.4880\n",
      "Episode: 179 Total reward: 51.0 Training loss: 14.4725 Explore P: 0.4861\n",
      "Episode: 180 Total reward: 71.0 Training loss: 6.2917 Explore P: 0.4849\n",
      "Model Saved\n",
      "Episode: 181 Total reward: 94.0 Training loss: 6.7129 Explore P: 0.4846\n",
      "Episode: 182 Total reward: 95.0 Training loss: 7.3987 Explore P: 0.4843\n",
      "Episode: 183 Total reward: 89.0 Training loss: 8.4959 Explore P: 0.4838\n",
      "Episode: 184 Total reward: -5.0 Training loss: 7.1187 Explore P: 0.4795\n",
      "Episode: 185 Total reward: 75.0 Training loss: 20.2008 Explore P: 0.4785\n",
      "Model Saved\n",
      "Episode: 186 Total reward: 95.0 Training loss: 7.2464 Explore P: 0.4782\n",
      "Episode: 187 Total reward: 67.0 Training loss: 17.0982 Explore P: 0.4766\n",
      "Episode: 188 Total reward: 55.0 Training loss: 68.9157 Explore P: 0.4747\n",
      "Episode: 189 Total reward: 94.0 Training loss: 7.3878 Explore P: 0.4744\n",
      "Episode: 190 Total reward: 93.0 Training loss: 10.7714 Explore P: 0.4740\n",
      "Model Saved\n",
      "Episode: 191 Total reward: 95.0 Training loss: 9.3598 Explore P: 0.4737\n",
      "Episode: 192 Total reward: 91.0 Training loss: 10.0108 Explore P: 0.4733\n",
      "Episode: 193 Total reward: 85.0 Training loss: 8.3679 Explore P: 0.4725\n",
      "Episode: 194 Total reward: 62.0 Training loss: 9.0950 Explore P: 0.4710\n",
      "Episode: 195 Total reward: 91.0 Training loss: 9.0522 Explore P: 0.4705\n",
      "Model Saved\n",
      "Episode: 196 Total reward: 95.0 Training loss: 15.6728 Explore P: 0.4702\n",
      "Episode: 197 Total reward: 93.0 Training loss: 5.9897 Explore P: 0.4699\n",
      "Episode: 198 Total reward: 74.0 Training loss: 11.3554 Explore P: 0.4688\n",
      "Episode: 199 Total reward: -13.0 Training loss: 7.1658 Explore P: 0.4648\n",
      "Episode: 200 Total reward: 65.0 Training loss: 14.5730 Explore P: 0.4634\n",
      "Model Saved\n",
      "Episode: 201 Total reward: 72.0 Training loss: 9.0965 Explore P: 0.4623\n",
      "Episode: 202 Total reward: 95.0 Training loss: 6.2009 Explore P: 0.4620\n",
      "Episode: 203 Total reward: 93.0 Training loss: 12.2334 Explore P: 0.4616\n",
      "Episode: 205 Total reward: 48.0 Training loss: 9.7467 Explore P: 0.4552\n",
      "Model Saved\n",
      "Episode: 206 Total reward: 95.0 Training loss: 7.4701 Explore P: 0.4550\n",
      "Episode: 207 Total reward: 95.0 Training loss: 7.7928 Explore P: 0.4547\n",
      "Episode: 208 Total reward: 95.0 Training loss: 8.1146 Explore P: 0.4544\n",
      "Episode: 209 Total reward: 95.0 Training loss: 7.3611 Explore P: 0.4542\n",
      "Episode: 210 Total reward: 94.0 Training loss: 9.1036 Explore P: 0.4539\n",
      "Model Saved\n",
      "Episode: 211 Total reward: 84.0 Training loss: 8.4264 Explore P: 0.4531\n",
      "Episode: 212 Total reward: 94.0 Training loss: 10.7247 Explore P: 0.4528\n",
      "Episode: 214 Total reward: 11.0 Training loss: 20.4153 Explore P: 0.4451\n",
      "Episode: 215 Total reward: 95.0 Training loss: 14.6704 Explore P: 0.4449\n",
      "Model Saved\n",
      "Episode: 216 Total reward: 47.0 Training loss: 10.5902 Explore P: 0.4429\n",
      "Episode: 217 Total reward: 94.0 Training loss: 8.9148 Explore P: 0.4426\n",
      "Episode: 219 Total reward: 92.0 Training loss: 27.2876 Explore P: 0.4379\n",
      "Episode: 220 Total reward: 21.0 Training loss: 24.7474 Explore P: 0.4352\n",
      "Model Saved\n",
      "Episode: 221 Total reward: 95.0 Training loss: 4.1244 Explore P: 0.4349\n",
      "Episode: 222 Total reward: 95.0 Training loss: 11.6306 Explore P: 0.4347\n",
      "Episode: 223 Total reward: 95.0 Training loss: 6.3149 Explore P: 0.4344\n",
      "Episode: 224 Total reward: 95.0 Training loss: 11.2138 Explore P: 0.4342\n",
      "Episode: 225 Total reward: 93.0 Training loss: 6.7293 Explore P: 0.4338\n",
      "Model Saved\n",
      "Episode: 226 Total reward: -7.0 Training loss: 18.3315 Explore P: 0.4301\n",
      "Episode: 228 Total reward: 95.0 Training loss: 7.8461 Explore P: 0.4257\n",
      "Episode: 229 Total reward: 89.0 Training loss: 12.2387 Explore P: 0.4252\n",
      "Episode: 230 Total reward: 91.0 Training loss: 8.6111 Explore P: 0.4248\n",
      "Model Saved\n",
      "Episode: 231 Total reward: 95.0 Training loss: 7.0694 Explore P: 0.4245\n",
      "Episode: 232 Total reward: 24.0 Training loss: 7.9144 Explore P: 0.4220\n",
      "Episode: 233 Total reward: 67.0 Training loss: 7.7917 Explore P: 0.4208\n",
      "Episode: 235 Total reward: 90.0 Training loss: 7.4014 Explore P: 0.4162\n",
      "Model Saved\n",
      "Episode: 236 Total reward: 94.0 Training loss: 9.4494 Explore P: 0.4159\n",
      "Episode: 237 Total reward: 66.0 Training loss: 22.5413 Explore P: 0.4147\n",
      "Episode: 238 Total reward: 95.0 Training loss: 8.4224 Explore P: 0.4145\n",
      "Episode: 239 Total reward: 94.0 Training loss: 10.2018 Explore P: 0.4142\n",
      "Episode: 240 Total reward: 95.0 Training loss: 18.8484 Explore P: 0.4140\n",
      "Model Saved\n",
      "Episode: 241 Total reward: 95.0 Training loss: 9.6716 Explore P: 0.4137\n",
      "Episode: 242 Total reward: 93.0 Training loss: 14.3615 Explore P: 0.4134\n",
      "Episode: 243 Total reward: 89.0 Training loss: 6.7192 Explore P: 0.4129\n",
      "Episode: 244 Total reward: 92.0 Training loss: 9.7390 Explore P: 0.4125\n",
      "Episode: 245 Total reward: 92.0 Training loss: 8.9627 Explore P: 0.4122\n",
      "Model Saved\n",
      "Episode: 246 Total reward: 94.0 Training loss: 13.3916 Explore P: 0.4119\n",
      "Episode: 247 Total reward: 95.0 Training loss: 10.2167 Explore P: 0.4117\n",
      "Episode: 248 Total reward: 49.0 Training loss: 7.2858 Explore P: 0.4100\n",
      "Episode: 249 Total reward: 93.0 Training loss: 9.4956 Explore P: 0.4097\n",
      "Episode: 250 Total reward: 94.0 Training loss: 8.6368 Explore P: 0.4094\n",
      "Model Saved\n",
      "Episode: 251 Total reward: 90.0 Training loss: 8.1944 Explore P: 0.4089\n",
      "Episode: 252 Total reward: 67.0 Training loss: 14.6603 Explore P: 0.4078\n",
      "Episode: 253 Total reward: 94.0 Training loss: 11.7068 Explore P: 0.4075\n",
      "Episode: 254 Total reward: 69.0 Training loss: 12.2074 Explore P: 0.4064\n",
      "Model Saved\n",
      "Episode: 256 Total reward: 40.0 Training loss: 7.9165 Explore P: 0.4005\n",
      "Episode: 257 Total reward: 94.0 Training loss: 7.0587 Explore P: 0.4002\n",
      "Episode: 258 Total reward: 13.0 Training loss: 5.6396 Explore P: 0.3974\n",
      "Episode: 259 Total reward: 95.0 Training loss: 11.5090 Explore P: 0.3971\n",
      "Episode: 260 Total reward: 94.0 Training loss: 5.4453 Explore P: 0.3969\n",
      "Model Saved\n",
      "Episode: 261 Total reward: 61.0 Training loss: 13.7899 Explore P: 0.3955\n",
      "Episode: 263 Total reward: 93.0 Training loss: 9.3516 Explore P: 0.3914\n",
      "Episode: 264 Total reward: 30.0 Training loss: 3.6590 Explore P: 0.3893\n",
      "Episode: 265 Total reward: 33.0 Training loss: 12.7761 Explore P: 0.3871\n",
      "Model Saved\n",
      "Episode: 266 Total reward: 61.0 Training loss: 8.9357 Explore P: 0.3857\n",
      "Episode: 267 Total reward: 95.0 Training loss: 12.0668 Explore P: 0.3855\n",
      "Episode: 268 Total reward: 94.0 Training loss: 7.8539 Explore P: 0.3853\n",
      "Episode: 269 Total reward: 92.0 Training loss: 9.7194 Explore P: 0.3849\n",
      "Episode: 270 Total reward: 66.0 Training loss: 8.4257 Explore P: 0.3838\n",
      "Model Saved\n",
      "Episode: 271 Total reward: 95.0 Training loss: 58.9040 Explore P: 0.3836\n",
      "Episode: 272 Total reward: 94.0 Training loss: 9.0761 Explore P: 0.3833\n",
      "Episode: 273 Total reward: 95.0 Training loss: 14.2150 Explore P: 0.3831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 274 Total reward: 93.0 Training loss: 9.7159 Explore P: 0.3828\n",
      "Episode: 275 Total reward: 95.0 Training loss: 13.4010 Explore P: 0.3826\n",
      "Model Saved\n",
      "Episode: 276 Total reward: 91.0 Training loss: 11.7094 Explore P: 0.3822\n",
      "Episode: 278 Total reward: 51.0 Training loss: 7.3124 Explore P: 0.3770\n",
      "Episode: 279 Total reward: 94.0 Training loss: 8.8203 Explore P: 0.3768\n",
      "Episode: 280 Total reward: 17.0 Training loss: 16.6367 Explore P: 0.3742\n",
      "Model Saved\n",
      "Episode: 281 Total reward: 95.0 Training loss: 10.2848 Explore P: 0.3740\n",
      "Episode: 282 Total reward: 93.0 Training loss: 9.4529 Explore P: 0.3737\n",
      "Episode: 283 Total reward: 25.0 Training loss: 6.3788 Explore P: 0.3715\n",
      "Episode: 284 Total reward: 66.0 Training loss: 9.5051 Explore P: 0.3704\n",
      "Episode: 285 Total reward: 68.0 Training loss: 13.6119 Explore P: 0.3694\n",
      "Model Saved\n",
      "Episode: 286 Total reward: 94.0 Training loss: 17.8033 Explore P: 0.3692\n",
      "Episode: 287 Total reward: 82.0 Training loss: 7.6668 Explore P: 0.3685\n",
      "Episode: 288 Total reward: 90.0 Training loss: 7.4030 Explore P: 0.3681\n",
      "Episode: 289 Total reward: 95.0 Training loss: 13.1549 Explore P: 0.3679\n",
      "Episode: 290 Total reward: 95.0 Training loss: 9.7160 Explore P: 0.3677\n",
      "Model Saved\n",
      "Episode: 291 Total reward: 95.0 Training loss: 13.4090 Explore P: 0.3675\n",
      "Episode: 292 Total reward: 94.0 Training loss: 10.7662 Explore P: 0.3672\n",
      "Episode: 293 Total reward: 94.0 Training loss: 12.7999 Explore P: 0.3670\n",
      "Episode: 294 Total reward: 89.0 Training loss: 9.1277 Explore P: 0.3665\n",
      "Model Saved\n",
      "Episode: 296 Total reward: 95.0 Training loss: 9.3529 Explore P: 0.3628\n",
      "Episode: 297 Total reward: 49.0 Training loss: 10.1345 Explore P: 0.3611\n",
      "Episode: 298 Total reward: 94.0 Training loss: 9.2147 Explore P: 0.3609\n",
      "Episode: 299 Total reward: 82.0 Training loss: 8.2857 Explore P: 0.3602\n",
      "Episode: 300 Total reward: 95.0 Training loss: 5.8907 Explore P: 0.3600\n",
      "Model Saved\n",
      "Episode: 302 Total reward: 76.0 Training loss: 8.6938 Explore P: 0.3558\n",
      "Episode: 303 Total reward: 92.0 Training loss: 7.5364 Explore P: 0.3555\n",
      "Episode: 304 Total reward: 73.0 Training loss: 9.1745 Explore P: 0.3547\n",
      "Episode: 305 Total reward: 95.0 Training loss: 6.7429 Explore P: 0.3545\n",
      "Model Saved\n",
      "Episode: 306 Total reward: 95.0 Training loss: 9.6271 Explore P: 0.3543\n",
      "Episode: 307 Total reward: 95.0 Training loss: 8.5832 Explore P: 0.3541\n",
      "Episode: 308 Total reward: 95.0 Training loss: 7.2566 Explore P: 0.3539\n",
      "Episode: 309 Total reward: 95.0 Training loss: 4.6156 Explore P: 0.3537\n",
      "Episode: 310 Total reward: 36.0 Training loss: 9.3830 Explore P: 0.3518\n",
      "Model Saved\n",
      "Episode: 311 Total reward: 45.0 Training loss: 11.4203 Explore P: 0.3502\n",
      "Episode: 312 Total reward: 92.0 Training loss: 9.0341 Explore P: 0.3499\n",
      "Episode: 313 Total reward: 94.0 Training loss: 6.4888 Explore P: 0.3497\n",
      "Episode: 314 Total reward: 95.0 Training loss: 7.3074 Explore P: 0.3495\n",
      "Episode: 315 Total reward: 95.0 Training loss: 8.4436 Explore P: 0.3493\n",
      "Model Saved\n",
      "Episode: 316 Total reward: 22.0 Training loss: 8.4433 Explore P: 0.3471\n",
      "Episode: 317 Total reward: 90.0 Training loss: 9.5296 Explore P: 0.3467\n",
      "Episode: 318 Total reward: 95.0 Training loss: 7.3915 Explore P: 0.3465\n",
      "Episode: 319 Total reward: 95.0 Training loss: 9.2824 Explore P: 0.3463\n",
      "Episode: 320 Total reward: 94.0 Training loss: 8.3721 Explore P: 0.3461\n",
      "Model Saved\n",
      "Episode: 321 Total reward: 20.0 Training loss: 11.7029 Explore P: 0.3439\n",
      "Episode: 322 Total reward: 40.0 Training loss: 6.9523 Explore P: 0.3422\n",
      "Episode: 323 Total reward: 30.0 Training loss: 6.8571 Explore P: 0.3402\n",
      "Episode: 324 Total reward: 94.0 Training loss: 8.4398 Explore P: 0.3399\n",
      "Episode: 325 Total reward: 36.0 Training loss: 7.9075 Explore P: 0.3381\n",
      "Model Saved\n",
      "Episode: 326 Total reward: 95.0 Training loss: 9.3094 Explore P: 0.3379\n",
      "Episode: 327 Total reward: 95.0 Training loss: 12.9682 Explore P: 0.3377\n",
      "Episode: 329 Total reward: 93.0 Training loss: 7.3573 Explore P: 0.3342\n",
      "Episode: 330 Total reward: 68.0 Training loss: 5.6380 Explore P: 0.3333\n",
      "Model Saved\n",
      "Episode: 331 Total reward: 95.0 Training loss: 10.9019 Explore P: 0.3331\n",
      "Episode: 332 Total reward: 94.0 Training loss: 12.8964 Explore P: 0.3329\n",
      "Episode: 333 Total reward: 94.0 Training loss: 7.5365 Explore P: 0.3327\n",
      "Episode: 334 Total reward: 92.0 Training loss: 14.2687 Explore P: 0.3324\n",
      "Episode: 335 Total reward: 57.0 Training loss: 6.1953 Explore P: 0.3311\n",
      "Model Saved\n",
      "Episode: 336 Total reward: 45.0 Training loss: 4.9803 Explore P: 0.3296\n",
      "Episode: 337 Total reward: 0.0 Training loss: 8.3492 Explore P: 0.3272\n",
      "Episode: 338 Total reward: 52.0 Training loss: 5.6634 Explore P: 0.3258\n",
      "Episode: 339 Total reward: 62.0 Training loss: 9.0290 Explore P: 0.3248\n",
      "Episode: 340 Total reward: 95.0 Training loss: 10.1210 Explore P: 0.3246\n",
      "Model Saved\n",
      "Episode: 341 Total reward: 95.0 Training loss: 4.6466 Explore P: 0.3244\n",
      "Episode: 342 Total reward: 0.0 Training loss: 10.1676 Explore P: 0.3217\n",
      "Episode: 343 Total reward: 95.0 Training loss: 13.7232 Explore P: 0.3215\n",
      "Episode: 344 Total reward: 95.0 Training loss: 12.8163 Explore P: 0.3213\n",
      "Episode: 345 Total reward: 65.0 Training loss: 116.5814 Explore P: 0.3204\n",
      "Model Saved\n",
      "Episode: 346 Total reward: 95.0 Training loss: 11.9533 Explore P: 0.3202\n",
      "Episode: 347 Total reward: 76.0 Training loss: 9.0000 Explore P: 0.3195\n",
      "Episode: 348 Total reward: 57.0 Training loss: 7.6891 Explore P: 0.3185\n",
      "Episode: 349 Total reward: 46.0 Training loss: 11.9967 Explore P: 0.3171\n",
      "Episode: 350 Total reward: 95.0 Training loss: 35.0867 Explore P: 0.3169\n",
      "Model Saved\n",
      "Episode: 351 Total reward: 7.0 Training loss: 5.4792 Explore P: 0.3147\n",
      "Episode: 352 Total reward: 95.0 Training loss: 37.5935 Explore P: 0.3145\n",
      "Episode: 353 Total reward: 95.0 Training loss: 6.9733 Explore P: 0.3143\n",
      "Episode: 354 Total reward: 47.0 Training loss: 33.3756 Explore P: 0.3130\n",
      "Episode: 355 Total reward: 94.0 Training loss: 5.4713 Explore P: 0.3127\n",
      "Model Saved\n",
      "Episode: 356 Total reward: 95.0 Training loss: 15.4747 Explore P: 0.3126\n",
      "Episode: 357 Total reward: 95.0 Training loss: 10.6133 Explore P: 0.3124\n",
      "Episode: 358 Total reward: 76.0 Training loss: 15.7496 Explore P: 0.3118\n",
      "Episode: 359 Total reward: 60.0 Training loss: 7.3263 Explore P: 0.3107\n",
      "Episode: 360 Total reward: 64.0 Training loss: 17.9268 Explore P: 0.3097\n",
      "Model Saved\n",
      "Episode: 362 Total reward: 70.0 Training loss: 11.2205 Explore P: 0.3060\n",
      "Episode: 363 Total reward: 94.0 Training loss: 8.5842 Explore P: 0.3058\n",
      "Episode: 364 Total reward: 95.0 Training loss: 13.0720 Explore P: 0.3056\n",
      "Episode: 365 Total reward: 72.0 Training loss: 7.2471 Explore P: 0.3049\n",
      "Model Saved\n",
      "Episode: 366 Total reward: 95.0 Training loss: 10.5945 Explore P: 0.3047\n",
      "Episode: 367 Total reward: 74.0 Training loss: 13.4495 Explore P: 0.3041\n",
      "Episode: 368 Total reward: 94.0 Training loss: 13.5821 Explore P: 0.3039\n",
      "Episode: 369 Total reward: 95.0 Training loss: 11.4229 Explore P: 0.3037\n",
      "Episode: 370 Total reward: 95.0 Training loss: 17.7659 Explore P: 0.3035\n",
      "Model Saved\n",
      "Episode: 371 Total reward: 95.0 Training loss: 14.7735 Explore P: 0.3033\n",
      "Episode: 372 Total reward: 62.0 Training loss: 12.3148 Explore P: 0.3023\n",
      "Episode: 373 Total reward: 49.0 Training loss: 26.1379 Explore P: 0.3011\n",
      "Episode: 374 Total reward: 95.0 Training loss: 8.4534 Explore P: 0.3009\n",
      "Episode: 375 Total reward: 71.0 Training loss: 12.8131 Explore P: 0.3002\n",
      "Model Saved\n",
      "Episode: 376 Total reward: 40.0 Training loss: 5.3560 Explore P: 0.2987\n",
      "Episode: 377 Total reward: 52.0 Training loss: 11.7557 Explore P: 0.2975\n",
      "Episode: 378 Total reward: 95.0 Training loss: 5.5538 Explore P: 0.2973\n",
      "Episode: 379 Total reward: 95.0 Training loss: 6.5316 Explore P: 0.2971\n",
      "Episode: 380 Total reward: 95.0 Training loss: 6.4136 Explore P: 0.2969\n",
      "Model Saved\n",
      "Episode: 381 Total reward: 95.0 Training loss: 8.7483 Explore P: 0.2968\n",
      "Episode: 382 Total reward: 27.0 Training loss: 12.2049 Explore P: 0.2951\n",
      "Episode: 384 Total reward: 88.0 Training loss: 8.1522 Explore P: 0.2919\n",
      "Episode: 385 Total reward: 49.0 Training loss: 7.3587 Explore P: 0.2907\n",
      "Model Saved\n",
      "Episode: 386 Total reward: 95.0 Training loss: 6.5402 Explore P: 0.2905\n",
      "Episode: 387 Total reward: 67.0 Training loss: 11.3640 Explore P: 0.2897\n",
      "Episode: 388 Total reward: 94.0 Training loss: 7.6259 Explore P: 0.2895\n",
      "Episode: 389 Total reward: 47.0 Training loss: 8.6576 Explore P: 0.2883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 390 Total reward: 95.0 Training loss: 13.1551 Explore P: 0.2881\n",
      "Model Saved\n",
      "Episode: 391 Total reward: 95.0 Training loss: 9.0346 Explore P: 0.2880\n",
      "Episode: 392 Total reward: 95.0 Training loss: 9.2041 Explore P: 0.2878\n",
      "Episode: 393 Total reward: 53.0 Training loss: 10.1196 Explore P: 0.2866\n",
      "Episode: 394 Total reward: 95.0 Training loss: 14.3436 Explore P: 0.2864\n",
      "Episode: 395 Total reward: 41.0 Training loss: 6.9891 Explore P: 0.2851\n",
      "Model Saved\n",
      "Episode: 396 Total reward: 93.0 Training loss: 37.0162 Explore P: 0.2848\n",
      "Episode: 397 Total reward: 95.0 Training loss: 8.7482 Explore P: 0.2847\n",
      "Episode: 398 Total reward: 95.0 Training loss: 12.4865 Explore P: 0.2845\n",
      "Episode: 399 Total reward: 55.0 Training loss: 10.0174 Explore P: 0.2834\n",
      "Episode: 400 Total reward: 50.0 Training loss: 9.2013 Explore P: 0.2823\n",
      "Model Saved\n",
      "Episode: 401 Total reward: 89.0 Training loss: 11.5834 Explore P: 0.2819\n",
      "Episode: 402 Total reward: 50.0 Training loss: 9.2483 Explore P: 0.2808\n",
      "Episode: 403 Total reward: 95.0 Training loss: 7.0971 Explore P: 0.2807\n",
      "Episode: 404 Total reward: 94.0 Training loss: 5.5279 Explore P: 0.2805\n",
      "Episode: 405 Total reward: 94.0 Training loss: 5.2272 Explore P: 0.2803\n",
      "Model Saved\n",
      "Episode: 406 Total reward: 95.0 Training loss: 33.5009 Explore P: 0.2801\n",
      "Episode: 407 Total reward: 92.0 Training loss: 25.1365 Explore P: 0.2799\n",
      "Episode: 408 Total reward: 95.0 Training loss: 15.1379 Explore P: 0.2797\n",
      "Episode: 409 Total reward: 95.0 Training loss: 8.6866 Explore P: 0.2796\n",
      "Episode: 410 Total reward: 94.0 Training loss: 11.3052 Explore P: 0.2794\n",
      "Model Saved\n",
      "Episode: 411 Total reward: 95.0 Training loss: 13.1651 Explore P: 0.2792\n",
      "Episode: 412 Total reward: 56.0 Training loss: 9.3825 Explore P: 0.2781\n",
      "Episode: 413 Total reward: 24.0 Training loss: 8.1642 Explore P: 0.2765\n",
      "Episode: 414 Total reward: 93.0 Training loss: 8.9425 Explore P: 0.2763\n",
      "Episode: 415 Total reward: 95.0 Training loss: 184.6464 Explore P: 0.2761\n",
      "Model Saved\n",
      "Episode: 416 Total reward: 42.0 Training loss: 12.4772 Explore P: 0.2748\n",
      "Episode: 417 Total reward: 67.0 Training loss: 10.1251 Explore P: 0.2740\n",
      "Episode: 418 Total reward: 95.0 Training loss: 6.0093 Explore P: 0.2739\n",
      "Episode: 419 Total reward: 95.0 Training loss: 11.9083 Explore P: 0.2737\n",
      "Episode: 420 Total reward: 74.0 Training loss: 8.8025 Explore P: 0.2731\n",
      "Model Saved\n",
      "Episode: 421 Total reward: 78.0 Training loss: 5.1943 Explore P: 0.2725\n",
      "Episode: 422 Total reward: 64.0 Training loss: 10.7076 Explore P: 0.2717\n",
      "Episode: 423 Total reward: 57.0 Training loss: 9.3582 Explore P: 0.2708\n",
      "Episode: 424 Total reward: 83.0 Training loss: 8.7196 Explore P: 0.2703\n",
      "Episode: 425 Total reward: 94.0 Training loss: 8.4886 Explore P: 0.2702\n",
      "Model Saved\n",
      "Episode: 426 Total reward: 92.0 Training loss: 12.2638 Explore P: 0.2699\n",
      "Episode: 427 Total reward: 92.0 Training loss: 14.3266 Explore P: 0.2697\n",
      "Episode: 428 Total reward: 95.0 Training loss: 5.9542 Explore P: 0.2695\n",
      "Episode: 429 Total reward: 94.0 Training loss: 5.4488 Explore P: 0.2694\n",
      "Episode: 430 Total reward: 94.0 Training loss: 37.2393 Explore P: 0.2692\n",
      "Model Saved\n",
      "Episode: 431 Total reward: 48.0 Training loss: 9.5754 Explore P: 0.2681\n",
      "Episode: 432 Total reward: 95.0 Training loss: 35.5186 Explore P: 0.2679\n",
      "Episode: 433 Total reward: 48.0 Training loss: 4.4163 Explore P: 0.2667\n",
      "Episode: 434 Total reward: 95.0 Training loss: 5.3200 Explore P: 0.2665\n",
      "Episode: 435 Total reward: 65.0 Training loss: 13.1538 Explore P: 0.2657\n",
      "Model Saved\n",
      "Episode: 436 Total reward: 75.0 Training loss: 6.9138 Explore P: 0.2652\n",
      "Episode: 437 Total reward: 95.0 Training loss: 12.1812 Explore P: 0.2650\n",
      "Episode: 438 Total reward: 66.0 Training loss: 19.1868 Explore P: 0.2643\n",
      "Episode: 439 Total reward: 95.0 Training loss: 9.6754 Explore P: 0.2641\n",
      "Episode: 440 Total reward: 94.0 Training loss: 10.7035 Explore P: 0.2639\n",
      "Model Saved\n",
      "Episode: 441 Total reward: 95.0 Training loss: 9.6514 Explore P: 0.2638\n",
      "Episode: 442 Total reward: 71.0 Training loss: 6.9456 Explore P: 0.2632\n",
      "Episode: 443 Total reward: 95.0 Training loss: 11.5850 Explore P: 0.2630\n",
      "Episode: 444 Total reward: 95.0 Training loss: 11.1369 Explore P: 0.2629\n",
      "Episode: 445 Total reward: 95.0 Training loss: 12.0561 Explore P: 0.2627\n",
      "Model Saved\n",
      "Episode: 446 Total reward: 66.0 Training loss: 6.2121 Explore P: 0.2619\n",
      "Episode: 447 Total reward: 94.0 Training loss: 11.4204 Explore P: 0.2618\n",
      "Episode: 448 Total reward: 95.0 Training loss: 7.8667 Explore P: 0.2616\n",
      "Episode: 449 Total reward: 95.0 Training loss: 10.6869 Explore P: 0.2615\n",
      "Episode: 450 Total reward: 94.0 Training loss: 4.1193 Explore P: 0.2613\n",
      "Model Saved\n",
      "Episode: 451 Total reward: 95.0 Training loss: 10.1720 Explore P: 0.2611\n",
      "Episode: 452 Total reward: 67.0 Training loss: 6.3608 Explore P: 0.2604\n",
      "Episode: 453 Total reward: 49.0 Training loss: 6.9353 Explore P: 0.2594\n",
      "Episode: 454 Total reward: 94.0 Training loss: 18.8967 Explore P: 0.2592\n",
      "Episode: 455 Total reward: 33.0 Training loss: 6.1464 Explore P: 0.2577\n",
      "Model Saved\n",
      "Episode: 456 Total reward: 95.0 Training loss: 8.0547 Explore P: 0.2576\n",
      "Episode: 457 Total reward: 42.0 Training loss: 8.3006 Explore P: 0.2564\n",
      "Episode: 458 Total reward: 93.0 Training loss: 6.1962 Explore P: 0.2562\n",
      "Episode: 459 Total reward: 91.0 Training loss: 9.1769 Explore P: 0.2559\n",
      "Episode: 460 Total reward: 52.0 Training loss: 11.4591 Explore P: 0.2549\n",
      "Model Saved\n",
      "Episode: 461 Total reward: 90.0 Training loss: 11.5095 Explore P: 0.2546\n",
      "Episode: 462 Total reward: 62.0 Training loss: 6.9546 Explore P: 0.2536\n",
      "Episode: 463 Total reward: 93.0 Training loss: 9.0412 Explore P: 0.2534\n",
      "Episode: 464 Total reward: 95.0 Training loss: 7.9957 Explore P: 0.2533\n",
      "Episode: 465 Total reward: 94.0 Training loss: 8.3003 Explore P: 0.2531\n",
      "Model Saved\n",
      "Episode: 466 Total reward: 95.0 Training loss: 9.8158 Explore P: 0.2530\n",
      "Episode: 467 Total reward: 94.0 Training loss: 9.7898 Explore P: 0.2528\n",
      "Episode: 468 Total reward: 38.0 Training loss: 5.6252 Explore P: 0.2515\n",
      "Episode: 469 Total reward: 93.0 Training loss: 7.5802 Explore P: 0.2513\n",
      "Episode: 470 Total reward: 87.0 Training loss: 6.4824 Explore P: 0.2510\n",
      "Model Saved\n",
      "Episode: 471 Total reward: 94.0 Training loss: 8.1126 Explore P: 0.2508\n",
      "Episode: 472 Total reward: 94.0 Training loss: 5.8036 Explore P: 0.2507\n",
      "Episode: 473 Total reward: 51.0 Training loss: 34.8951 Explore P: 0.2497\n",
      "Episode: 474 Total reward: 93.0 Training loss: 6.7843 Explore P: 0.2495\n",
      "Episode: 475 Total reward: 95.0 Training loss: 6.8093 Explore P: 0.2494\n",
      "Model Saved\n",
      "Episode: 476 Total reward: 95.0 Training loss: 6.3043 Explore P: 0.2492\n",
      "Episode: 477 Total reward: 68.0 Training loss: 5.7681 Explore P: 0.2486\n",
      "Episode: 478 Total reward: 59.0 Training loss: 13.2652 Explore P: 0.2477\n",
      "Episode: 479 Total reward: 94.0 Training loss: 23.1225 Explore P: 0.2475\n",
      "Episode: 480 Total reward: 91.0 Training loss: 14.5350 Explore P: 0.2473\n",
      "Model Saved\n",
      "Episode: 481 Total reward: 94.0 Training loss: 7.6804 Explore P: 0.2471\n",
      "Episode: 482 Total reward: 50.0 Training loss: 11.3463 Explore P: 0.2461\n",
      "Episode: 483 Total reward: 95.0 Training loss: 10.2281 Explore P: 0.2460\n",
      "Episode: 484 Total reward: 94.0 Training loss: 12.4678 Explore P: 0.2458\n",
      "Episode: 485 Total reward: 94.0 Training loss: 12.0741 Explore P: 0.2457\n",
      "Model Saved\n",
      "Episode: 486 Total reward: 67.0 Training loss: 10.1352 Explore P: 0.2450\n",
      "Episode: 487 Total reward: 56.0 Training loss: 8.7306 Explore P: 0.2442\n",
      "Episode: 488 Total reward: 75.0 Training loss: 5.7484 Explore P: 0.2437\n",
      "Episode: 489 Total reward: 93.0 Training loss: 10.9196 Explore P: 0.2435\n",
      "Episode: 490 Total reward: 51.0 Training loss: 11.4940 Explore P: 0.2425\n",
      "Model Saved\n",
      "Episode: 491 Total reward: 95.0 Training loss: 9.8628 Explore P: 0.2424\n",
      "Episode: 492 Total reward: 95.0 Training loss: 13.1711 Explore P: 0.2423\n",
      "Episode: 493 Total reward: 90.0 Training loss: 5.8078 Explore P: 0.2420\n",
      "Episode: 494 Total reward: 94.0 Training loss: 11.8072 Explore P: 0.2419\n",
      "Episode: 495 Total reward: 40.0 Training loss: 9.8057 Explore P: 0.2407\n",
      "Model Saved\n",
      "Episode: 496 Total reward: 95.0 Training loss: 19.3631 Explore P: 0.2405\n",
      "Episode: 497 Total reward: 46.0 Training loss: 6.9127 Explore P: 0.2395\n",
      "Episode: 498 Total reward: 93.0 Training loss: 11.5259 Explore P: 0.2393\n",
      "Episode: 499 Total reward: 93.0 Training loss: 6.1575 Explore P: 0.2391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 500 Total reward: 94.0 Training loss: 7.7699 Explore P: 0.2390\n",
      "Model Saved\n",
      "Episode: 501 Total reward: 93.0 Training loss: 7.5197 Explore P: 0.2388\n",
      "Episode: 502 Total reward: 93.0 Training loss: 9.1131 Explore P: 0.2386\n",
      "Episode: 503 Total reward: 95.0 Training loss: 6.2553 Explore P: 0.2385\n",
      "Episode: 504 Total reward: 95.0 Training loss: 8.3049 Explore P: 0.2383\n",
      "Episode: 505 Total reward: 37.0 Training loss: 6.7135 Explore P: 0.2371\n",
      "Model Saved\n",
      "Episode: 506 Total reward: 27.0 Training loss: 9.1066 Explore P: 0.2358\n",
      "Episode: 507 Total reward: 93.0 Training loss: 7.5311 Explore P: 0.2356\n",
      "Episode: 508 Total reward: 49.0 Training loss: 7.5265 Explore P: 0.2346\n",
      "Episode: 509 Total reward: 26.0 Training loss: 7.2603 Explore P: 0.2332\n",
      "Episode: 510 Total reward: 95.0 Training loss: 15.0211 Explore P: 0.2331\n",
      "Model Saved\n",
      "Episode: 511 Total reward: 94.0 Training loss: 6.2787 Explore P: 0.2329\n",
      "Episode: 512 Total reward: 62.0 Training loss: 8.8246 Explore P: 0.2321\n",
      "Episode: 513 Total reward: 51.0 Training loss: 5.8527 Explore P: 0.2313\n",
      "Episode: 514 Total reward: 94.0 Training loss: 4.5104 Explore P: 0.2311\n",
      "Model Saved\n",
      "Episode: 516 Total reward: 95.0 Training loss: 7.6414 Explore P: 0.2288\n",
      "Episode: 517 Total reward: 95.0 Training loss: 4.1590 Explore P: 0.2286\n",
      "Episode: 518 Total reward: 25.0 Training loss: 9.4901 Explore P: 0.2273\n",
      "Episode: 519 Total reward: 46.0 Training loss: 7.6318 Explore P: 0.2263\n",
      "Episode: 520 Total reward: 94.0 Training loss: 9.5421 Explore P: 0.2262\n",
      "Model Saved\n",
      "Episode: 521 Total reward: 42.0 Training loss: 7.9531 Explore P: 0.2251\n",
      "Episode: 522 Total reward: 64.0 Training loss: 6.7734 Explore P: 0.2244\n",
      "Episode: 523 Total reward: 95.0 Training loss: 6.8548 Explore P: 0.2243\n",
      "Episode: 524 Total reward: 91.0 Training loss: 7.9930 Explore P: 0.2241\n",
      "Episode: 525 Total reward: 91.0 Training loss: 8.8868 Explore P: 0.2239\n",
      "Model Saved\n",
      "Episode: 526 Total reward: 26.0 Training loss: 6.6766 Explore P: 0.2225\n",
      "Episode: 527 Total reward: 94.0 Training loss: 8.2727 Explore P: 0.2223\n",
      "Episode: 528 Total reward: 75.0 Training loss: 8.6970 Explore P: 0.2219\n",
      "Episode: 529 Total reward: 95.0 Training loss: 10.6074 Explore P: 0.2218\n",
      "Episode: 530 Total reward: 49.0 Training loss: 8.0362 Explore P: 0.2209\n",
      "Model Saved\n",
      "Episode: 531 Total reward: 95.0 Training loss: 10.6298 Explore P: 0.2208\n",
      "Episode: 532 Total reward: 93.0 Training loss: 6.2071 Explore P: 0.2206\n",
      "Episode: 533 Total reward: 95.0 Training loss: 9.4483 Explore P: 0.2205\n",
      "Episode: 534 Total reward: 86.0 Training loss: 9.1492 Explore P: 0.2201\n",
      "Episode: 535 Total reward: 93.0 Training loss: 7.6517 Explore P: 0.2200\n",
      "Model Saved\n",
      "Episode: 536 Total reward: 94.0 Training loss: 8.5433 Explore P: 0.2198\n",
      "Episode: 537 Total reward: 93.0 Training loss: 7.2054 Explore P: 0.2197\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0b3772920dbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;31m# Get Q values for next_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mtarget_Qs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDQNetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mDQNetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;31m# Set Qhat = r if the episode ends at +1, otherwise set Qhat = r + gamma*maxQ(s', a')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gameplai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gameplai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gameplai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gameplai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gameplai\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    rewards_list = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "\n",
    "        decay_step = 0\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            # Make new episode\n",
    "            game.new_episode()\n",
    "            step = 0\n",
    "\n",
    "            # Observe the first state\n",
    "            frame = game.get_state().screen_buffer\n",
    "            state = stack_frames(stacked_frames, frame)\n",
    "\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                # Increase decay_step\n",
    "                decay_step +=1\n",
    "\n",
    "                ## EPSILON GREEDY STRATEGY\n",
    "                # Choose action a from state s using epsilon greedy.\n",
    "                ## First we randomize a number\n",
    "                exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "                # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "                explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "\n",
    "\n",
    "                if (explore_probability > exp_exp_tradeoff):\n",
    "                    # Make a random action\n",
    "                    action = random.choice(possible_actions)\n",
    "\n",
    "                else:\n",
    "                    # Get action from Q-network\n",
    "                    # Estimate the Qs values state\n",
    "                    Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "\n",
    "                    # Take the biggest Q value (= the best action)\n",
    "                    action = np.argmax(Qs)\n",
    "\n",
    "                    action = possible_actions[int(action)]\n",
    "\n",
    "                # Do the action\n",
    "                reward = game.make_action(action)\n",
    "\n",
    "                # Look if the episode is finished\n",
    "                done = game.is_episode_finished()\n",
    "\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((84,84), dtype=np.int)\n",
    "                    next_state = stack_frames(stacked_frames, next_state)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    total_reward = game.get_total_reward()\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    rewards_list.append((episode, total_reward))\n",
    "\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                else:\n",
    "                    # Get the next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    next_state = stack_frames(stacked_frames, next_state)\n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                ### LEARNING PART            \n",
    "                # Obtain random mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions = np.array([each[1] for each in batch])\n",
    "                rewards = np.array([each[2] for each in batch]) \n",
    "                next_states = np.array([each[3] for each in batch])\n",
    "                dones = np.array([each[4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                # Get Q values for next_state \n",
    "                target_Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states})\n",
    "\n",
    "                # Set Qhat = r if the episode ends at +1, otherwise set Qhat = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones[i]\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards[i])\n",
    "                    else:\n",
    "                        target = rewards[i] + gamma * np.max(target_Qs[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "\n",
    "\n",
    "                targets = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "                                    feed_dict={DQNetwork.inputs_: states,\n",
    "                                               DQNetwork.target_Q: targets,\n",
    "                                               DQNetwork.actions_: actions})\n",
    "\n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states,\n",
    "                                                   DQNetwork.target_Q: targets,\n",
    "                                                   DQNetwork.actions_: actions})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "\n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Watch our Agent play üëÄ\n",
    "Now that we trained our agent, we can test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game = DoomGame()\n",
    "    \n",
    "    totalScore = 0\n",
    "    \n",
    "    # Load the correct configuration (test configuration)\n",
    "    game.load_config(\"basic_test.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case basic scenario)\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    for i in range(10):\n",
    "        \n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            frame = game.get_state().screen_buffer\n",
    "            state = stack_frames(stacked_frames, frame)\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            action = np.argmax(Qs)\n",
    "            action = possible_actions[int(action)]\n",
    "            game.make_action(action)        \n",
    "            score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "        totalScore += score\n",
    "    print(\"TOTAL_SCORE\", totalScore/100.0)\n",
    "    game.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
